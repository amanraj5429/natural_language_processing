{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_using_HAN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VO--cRfqrO7u"
      },
      "source": [
        "**To Upload the data from mydrive to this notebook**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ9XlS7-ZG6L"
      },
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjrZeGnirpMO",
        "outputId": "440661db-722c-4102-d8c6-6254a748672a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "link1 = 'https://drive.google.com/open?id=18PSgJ2wCNkwr9-DH-oWlsRmIUuCMDMde'\n",
        "fluff, id_train_unlabled = link1.split('=')\n",
        "print (id_train_unlabled) # Verify that you have everything after '='\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18PSgJ2wCNkwr9-DH-oWlsRmIUuCMDMde\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE80x-PurruV"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':id_train_unlabled}) \n",
        "downloaded.GetContentFile('train_unlabled.tsv')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKLDi3xQrz2E",
        "outputId": "30f2a02c-f4f7-4ab9-fe37-c71eb59f5665",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "link2 = 'https://drive.google.com/open?id=1KgxQWhq6L8IqcQRuvW-OQIDH6iuq4Lo9'\n",
        "fluff, id_train_labled = link2.split('=')\n",
        "print (id_train_labled) # Verify that you have everything after '='"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1KgxQWhq6L8IqcQRuvW-OQIDH6iuq4Lo9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EosUYUwor4j_"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':id_train_labled}) \n",
        "downloaded.GetContentFile('train_labled.tsv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwfpzqEtsAyn",
        "outputId": "555837a7-1fca-40e2-f9cc-dad06f2085cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "link3 = 'https://drive.google.com/open?id=1z56Wba5iVO9RYuU4NbnHtVVkRW4YfaT8'\n",
        "fluff, id_test = link3.split('=')\n",
        "print (id_test) # Verify that you have everything after '='"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1z56Wba5iVO9RYuU4NbnHtVVkRW4YfaT8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-40QNkBbsFa6"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':id_test}) \n",
        "downloaded.GetContentFile('test.tsv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwrAYcbNsKOs",
        "outputId": "94ccd13d-34a8-41b2-984b-5872521c4af2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "link4 = 'https://drive.google.com/open?id=1cTo4HOTu3M3bc-ve7ImPiK4CCbOYnGcO'\n",
        "fluff, id_sub = link4.split('=')\n",
        "print (id_sub) # Verify that you have everything after '='"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1cTo4HOTu3M3bc-ve7ImPiK4CCbOYnGcO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjmCQNzZsR8T"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':id_sub}) \n",
        "downloaded.GetContentFile('sub.csv')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8MBP4C4sYX2",
        "outputId": "e5c9f1ad-bbff-493f-dc56-546873617f69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-09 11:26:34--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-10-09 11:26:34--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-10-09 11:26:34--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.1’\n",
            "\n",
            "glove.6B.zip.1      100%[===================>] 822.24M  2.02MB/s    in 6m 28s  \n",
            "\n",
            "2020-10-09 11:33:02 (2.12 MB/s) - ‘glove.6B.zip.1’ saved [862182613/862182613]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mI6z5YHkzoKj"
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('glove.6B.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VFsBFZX2XsC"
      },
      "source": [
        "This example applies the HAN classifier to Kaggle's IMDB review dataset. The goal is to predict whether a review is positive (5 star rating >=3) or negative (otherwise)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6073mgra2hgs"
      },
      "source": [
        "**Importing some required library and functions for data processing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7zpco5L2YrH"
      },
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import logging\n",
        "import sys\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JqR1QyC2uD_"
      },
      "source": [
        "Using TensorFlow backend."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjfT_sx42vx5"
      },
      "source": [
        "# Create a logger to provide info on the state of the\n",
        "# script\n",
        "stdout = logging.StreamHandler(sys.stdout)\n",
        "stdout.setFormatter(logging.Formatter(\n",
        "    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "))\n",
        "logger = logging.getLogger('default')\n",
        "logger.setLevel(logging.INFO)\n",
        "logger.addHandler(stdout)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVVmpLoR21DF",
        "outputId": "f81e5641-d7c1-44b6-d9db-4300ad61ed1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#####################################################\n",
        "# Pre processing                                    #\n",
        "#####################################################\n",
        "logger.info(\"Pre-processsing data.\")\n",
        "\n",
        "# Load Kaggle's IMDB example data\n",
        "data = pd.read_csv('train_labled.tsv', sep='\\t')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-09 11:33:30,861 - default - INFO - Pre-processsing data.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxJJdlJJ3EKC",
        "outputId": "751208b8-3950-49d5-f14c-7d3257efd057",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sentiment</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5814_8</td>\n",
              "      <td>1</td>\n",
              "      <td>With all this stuff going down at the moment w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381_9</td>\n",
              "      <td>1</td>\n",
              "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7759_3</td>\n",
              "      <td>0</td>\n",
              "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3630_4</td>\n",
              "      <td>0</td>\n",
              "      <td>It must be assumed that those who praised this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>9495_8</td>\n",
              "      <td>1</td>\n",
              "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  sentiment                                             review\n",
              "0  5814_8          1  With all this stuff going down at the moment w...\n",
              "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
              "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
              "3  3630_4          0  It must be assumed that those who praised this...\n",
              "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI5-oz5H3GHI"
      },
      "source": [
        "# Do some basic cleaning of the review text\n",
        "def remove_quotations(text):\n",
        "    \"\"\"\n",
        "    Remove quotations and slashes from the dataset.\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"\\\\\", \"\", text)\n",
        "    text = re.sub(r\"\\'\", \"\", text)\n",
        "    text = re.sub(r\"\\\"\", \"\", text)\n",
        "    return text"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBWFB4zv3KfL"
      },
      "source": [
        "def remove_html(text):\n",
        "    \"\"\"\n",
        "    Very, very raw parser to remove HTML tags from\n",
        "    texts.\n",
        "    \"\"\"\n",
        "    tags_regex = re.compile(r'<.*?>')\n",
        "    return tags_regex.sub('', text)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFTNPFja3Rgd"
      },
      "source": [
        "data['review'] = data['review'].apply(remove_quotations)\n",
        "data['review'] = data['review'].apply(remove_html)\n",
        "data['review'] = data['review'].apply(lambda x: x.strip().lower())"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqtoiGMt3adE"
      },
      "source": [
        "# Get the data and the sentiment\n",
        "reviews = data['review'].values\n",
        "target = data['sentiment'].values"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5VRfC1Y3c4N",
        "outputId": "131bfb33-4248-48d7-bd21-d4f531052055",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "reviews[5:]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['i dont know why people think this is such a bad movie. its got a pretty good plot, some good action, and the change of location for harry does not hurt either. sure some of its offensive and gratuitous but this is not the only movie like that. eastwood is in good form as dirty harry, and i liked pat hingle in this movie as the small town cop. if you liked dirty harry, then you should see this one, its a lot better than the dead pool. 4/5',\n",
              "       'this movie could have been very good, but comes up way short. cheesy special effects and so-so acting. i could have looked past that if the story wasnt so lousy. if there was more of a background story, it would have been better. the plot centers around an evil druid witch who is linked to this woman who gets migraines. the movie drags on and on and never clearly explains anything, it just keeps plodding on. christopher walken has a part, but it is completely senseless, as is most of the movie. this movie had potential, but it looks like some really bad made for tv movie. i would avoid this movie.',\n",
              "       'i watched this video at a friends house. im glad i did not waste money buying this one. the video cover has a scene from the 1975 movie capricorn one. the movie starts out with several clips of rocket blow-ups, most not related to manned flight. sibrels smoking gun is a short video clip of the astronauts preparing a video broadcast. he edits in his own voice-over instead of letting us listen to what the crew had to say. the video curiously ends with a showing of the zapruder film. his claims about radiation, shielding, star photography, and others lead me to believe is he extremely ignorant or has some sort of ax to grind against nasa, the astronauts, or american in general. his science is bad, and so is this video.',\n",
              "       ...,\n",
              "       'guy is a loser. cant get girls, needs to build up, is picked on by stronger more successful guys, etc. seen it, saw it, moved on. id have to say that rob needs to move past the adam sandler part of his life. and get out of the adam sandler plots. there are two funny parts in the whole movie. i couldnt even finish the last 5 minutes. i was getting bored. the animal is an alright film. i do usually enjoy adam sandler films that have the same plot. but this was trying too hard to impress. the jokes are very old. so, trust me. this is not a film that most people could really get into. but some did, so ill be nice.3/10',\n",
              "       'this 30 minute documentary buñuel made in the early 1930s about one of spains poorest regions is, in my opinion, one of his weakest films. first, lets admit that 70 years later, spain is much richer than it was then (and when i say this, i fully admit that wealth can bring problems of its own, like excessive individualism and consumerism, though all in all wealth its a far better condition than the extreme poverty portrayed here). and if poverty receded in spain it was not exactly with the sort of socialism that buñuel favored, but with western european style capitalism. but one of the most shocking things about the movie is this: in one scene, the narrator chides that in school, children are taught the value of pi. teaching math to poor people, the horror!. buñuel shortsightedness is at its most glaring here, not realizing that it is access to the latest knowledge and technology what will help the poor overcome their situation. what is he proposing? that children are taught exactly what at school? doesnt buñuel understand that it is the lack of modern technology that has made them poor in comparison with other people?',\n",
              "       'i saw this movie as a child and it broke my heart! no other story had such a unfinished ending... i grew up on many great anime movies and this was one of my favourites, because it was so unusual - a story about unfairness, and cruelty, and loneliness, and life, and choices that cant be undone, and the need for others. chirin is made alone when the wolf kills his mother, but the wolf is alone, too, when chirin follows him into the mountain. the wolf doesnt kill the lamb, even though each night he says maybe ill eat you tomorrow. the tape of it i have is broken and degraded from age and use. i will repair it and watch the movie again someday and cry just as hard as i did as a child. stories like this, with this depth and feeling, and this intricacy of meaning, are very rare. it is a sad story, but ive never encountered any catharsis more beautifully made. i am glad i have seen this movie, and im glad i saw it as a child.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1aU1z3M6Brz",
        "outputId": "b5000571-e61a-459b-f04c-e5e14b32c173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "target[5:]"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 0, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZAk-oE66Hbu",
        "outputId": "b7087bda-b366-4eb8-e62c-10dbd98cd5f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(reviews)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR2K-O-f6UVf",
        "outputId": "055d55c6-fc6e-4206-9240-28309a216ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(target)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BJikUbi6cUI"
      },
      "source": [
        "MAX_WORDS_PER_SENT = 100\n",
        "MAX_SENT = 15\n",
        "MAX_VOC_SIZE = 20000\n",
        "GLOVE_DIM = 100\n",
        "TEST_SPLIT = 0.2"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD_ppluL6kIJ"
      },
      "source": [
        "del data"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRZDnG-p6p0v",
        "outputId": "e800853b-6064-4a95-9970-b11c2c1fd7cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#####################################################\n",
        "# Tokenization                                      #\n",
        "#####################################################\n",
        "logger.info(\"Tokenization.\")\n",
        "\n",
        "# Build a Keras Tokenizer that can encode every token\n",
        "word_tokenizer = Tokenizer(num_words=MAX_VOC_SIZE)\n",
        "word_tokenizer.fit_on_texts(reviews)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-09 11:33:31,889 - default - INFO - Tokenization.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJbsGM7h6vB_"
      },
      "source": [
        "# Construct the input matrix. This should be a nd-array of\n",
        "# shape (n_samples, MAX_SENT, MAX_WORDS_PER_SENT).\n",
        "# We zero-pad this matrix (this does not influence\n",
        "# any predictions due to the attention mechanism.\n",
        "X = np.zeros((len(reviews), MAX_SENT, MAX_WORDS_PER_SENT), dtype='int32')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltiEVKtv7Bpt",
        "outputId": "cfca1dce-d37c-4bfa-9dda-b89ea76eb858",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKH78aA66z_4"
      },
      "source": [
        "for i, review in enumerate(reviews):\n",
        "    sentences = sent_tokenize(review)\n",
        "    tokenized_sentences = word_tokenizer.texts_to_sequences(\n",
        "        sentences\n",
        "    )\n",
        "    tokenized_sentences = pad_sequences(\n",
        "        tokenized_sentences, maxlen=MAX_WORDS_PER_SENT\n",
        "    )\n",
        "\n",
        "    pad_size = MAX_SENT - tokenized_sentences.shape[0]\n",
        "\n",
        "    if pad_size < 0:\n",
        "        tokenized_sentences = tokenized_sentences[0:MAX_SENT]\n",
        "    else:\n",
        "        tokenized_sentences = np.pad(\n",
        "            tokenized_sentences, ((0,pad_size),(0,0)),\n",
        "            mode='constant', constant_values=0\n",
        "        )\n",
        "\n",
        "    # Store this observation as the i-th observation in\n",
        "    # the data matrix\n",
        "    X[i] = tokenized_sentences[None, ...]"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su0xUtPp8Nbn"
      },
      "source": [
        "# Transform the labels into a format Keras can handle\n",
        "y = to_categorical(target)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMVrKbw_8WX6"
      },
      "source": [
        "# We make a train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhsBcLiU8cQJ",
        "outputId": "219a2ed3-d37e-412d-9c77-121b85c89318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "X_train"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[    0,     0,     0, ...,     3,   112,   374],\n",
              "        [    0,     0,     0, ...,    14,    10,   213],\n",
              "        [    0,     0,     0, ...,    59,    65,   157],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0]],\n",
              "\n",
              "       [[    0,     0,     0, ...,    16,   121,    89],\n",
              "        [    0,     0,     0, ...,    61,  3539,   203],\n",
              "        [    0,     0,     0, ...,   121,    89,   808],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0]],\n",
              "\n",
              "       [[    0,     0,     0, ..., 17303,  1964,  3884],\n",
              "        [    0,     0,     0, ...,   328,     5,   600],\n",
              "        [    0,     0,     0, ...,     7,  7578, 11588],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[    0,     0,     0, ...,   114,  3601,   121],\n",
              "        [    0,     0,     0, ...,     3,  8295,    92],\n",
              "        [    0,     0,     0, ...,    32, 17079,   499],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0]],\n",
              "\n",
              "       [[    0,     0,     0, ...,    12,   424,  1222],\n",
              "        [    0,     0,     0, ...,   115,  6719,  7646],\n",
              "        [    0,     0,     0, ...,     4, 19483,     2],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0],\n",
              "        [    0,     0,     0, ...,     0,     0,     0]],\n",
              "\n",
              "       [[    0,     0,     0, ...,  5860,  1151,   550],\n",
              "        [    0,     0,     0, ...,    25,     3,   842],\n",
              "        [    0,     0,     0, ...,     4,   516,  2720],\n",
              "        ...,\n",
              "        [    0,     0,     0, ...,   122,    26,   180],\n",
              "        [    0,     0,     0, ...,  7774,  7183,   165],\n",
              "        [    0,     0,     0, ...,  1418,   115,     8]]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmQ6uzUD8fKz"
      },
      "source": [
        "**Word Embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ian7mL1M8-HW",
        "outputId": "0fda5fd6-975c-44ac-bdfa-c08ecaec3c54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Word Embeddings                                   #\n",
        "#####################################################\n",
        "logger.info(\n",
        "    \"Creating embedding matrix using pre-trained GloVe vectors.\"\n",
        ")\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-09 11:33:59,569 - default - INFO - Creating embedding matrix using pre-trained GloVe vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSoOZq6k8_tW"
      },
      "source": [
        "# Now, we need to build the embedding matrix. For this we use\n",
        "# a pretrained (on the wikipedia corpus) 100-dimensional GloVe\n",
        "# model.\n",
        "\n",
        "# Load the embeddings from a file\n",
        "embeddings = {}\n",
        "with open('glove.6B.%dd.txt' % GLOVE_DIM, encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "\n",
        "        embeddings[word] = coefs"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qCYndOt9Idl"
      },
      "source": [
        "# Initialize a matrix to hold the word embeddings\n",
        "embedding_matrix = np.random.random(\n",
        "    (len(word_tokenizer.word_index) + 1, GLOVE_DIM)\n",
        ")\n",
        "\n",
        "# Let the padded indices map to zero-vectors. This will\n",
        "# prevent the padding from influencing the results\n",
        "embedding_matrix[0] = 0"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J-_dh6e9Pn8"
      },
      "source": [
        "# Loop though all the words in the word_index and where possible\n",
        "# replace the random initalization with the GloVe vector.\n",
        "for word, index in word_tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[index] = embedding_vector\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bRnfUy79Xdf"
      },
      "source": [
        "**Now construct the Layers and Model**\n",
        "\n",
        "**1. Attention layer**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1Lx_EN79mCU"
      },
      "source": [
        "\"\"\"containing custom Keras layers that use the attention mechanism.\"\"\"\n",
        "\n",
        "import keras\n",
        "from keras import backend as K"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT4fAdOh9nIY"
      },
      "source": [
        "class AttentionLayer(keras.layers.Layer):\n",
        "    def __init__(self, context_vector_length=100, **kwargs):\n",
        "        \"\"\"\n",
        "        An implementation of a attention layer. This layer\n",
        "        accepts a 3d Tensor (batch_size, time_steps, input_dim) and\n",
        "        applies a single layer attention mechanism in the time\n",
        "        direction (the second axis).\n",
        "        :param context_vector_lenght: (int) The size of the hidden context vector.\n",
        "            If set to 1 this layer reduces to a standard attention layer.\n",
        "        :param kwargs: Any argument that the baseclass Layer accepts.\n",
        "        \"\"\"\n",
        "        self.context_vector_length = context_vector_length\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        dim = input_shape[2]\n",
        "\n",
        "        # Add a weights layer for the\n",
        "        self.W = self.add_weight(\n",
        "            name='W', shape=(dim, self.context_vector_length),\n",
        "            initializer=keras.initializers.get('uniform'),\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        self.u = self.add_weight(\n",
        "            name='context_vector', shape=(self.context_vector_length, 1),\n",
        "            initializer=keras.initializers.get('uniform'),\n",
        "            trainable=True\n",
        "        )\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def _get_attention_weights(self, X):\n",
        "        \"\"\"\n",
        "        Computes the attention weights for each timestep in X\n",
        "        :param X: 3d-tensor (batch_size, time_steps, input_dim)\n",
        "         :return: 2d-tensor (batch_size, time_steps) of attention weights\n",
        "        \"\"\"\n",
        "        # Compute a time-wise stimulus, i.e. a stimulus for each\n",
        "        # time step. For this first compute a hidden layer of\n",
        "        # dimension self.context_vector_length and take the\n",
        "        # similarity of this layer with self.u as the stimulus\n",
        "        u_tw = K.tanh(K.dot(X, self.W))\n",
        "        tw_stimulus = K.dot(u_tw, self.u)\n",
        "\n",
        "        # Remove the last axis an apply softmax to the stimulus to\n",
        "        # get a probability.\n",
        "        tw_stimulus = K.reshape(tw_stimulus, (-1, tw_stimulus.shape[1]))\n",
        "        att_weights = K.softmax(tw_stimulus)\n",
        "\n",
        "        return att_weights\n",
        "\n",
        "    def call(self, X):\n",
        "        att_weights = self._get_attention_weights(X)\n",
        "\n",
        "        # Reshape the attention weights to match the dimensions of X\n",
        "        att_weights = K.reshape(att_weights, (-1, att_weights.shape[1], 1))\n",
        "        att_weights = K.repeat_elements(att_weights, X.shape[-1], -1)\n",
        "\n",
        "        # Multiply each input by its attention weights\n",
        "        weighted_input = keras.layers.Multiply()([X, att_weights])\n",
        "\n",
        "        # Sum in the direction of the time-axis.\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape[0], input_shape[2]\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'context_vector_length': self.context_vector_length\n",
        "        }\n",
        "        base_config = super(AttentionLayer, self).get_config()\n",
        "        return {**base_config, **config}"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTgmuFD3-K2P"
      },
      "source": [
        "**2. Model preperation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEEf7oy3-N3c"
      },
      "source": [
        "import keras\n",
        "from keras.layers import (\n",
        "    Dense, GRU, TimeDistributed, Input,\n",
        "    Embedding, Bidirectional, Lambda\n",
        ")\n",
        "from keras.models import Model\n",
        "# from keras_han.layers import AttentionLayer"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EQVaUezO18L"
      },
      "source": [
        "class HAN(Model):\n",
        "    def __init__(\n",
        "            self, max_words, max_sentences, output_size,\n",
        "            embedding_matrix, word_encoding_dim=200,\n",
        "            sentence_encoding_dim=200, inputs=None,\n",
        "            outputs=None, name='han-for-docla'\n",
        "    ):\n",
        "        \"\"\"\n",
        "        A Keras implementation of Hierarchical Attention networks\n",
        "        for document classification.\n",
        "        :param max_words: The maximum number of words per sentence\n",
        "        :param max_sentences: The maximum number of sentences\n",
        "        :param output_size: The dimension of the last layer (i.e.\n",
        "            the number of classes you wish to predict)\n",
        "        :param embedding_matrix: The embedding matrix to use for\n",
        "            representing words\n",
        "        :param word_encoding_dim: The dimension of the GRU\n",
        "            layer in the word encoder.\n",
        "        :param sentence_encoding_dim: The dimension of the GRU\n",
        "            layer in the sentence encoder.\n",
        "        \"\"\"\n",
        "        self.max_words = max_words\n",
        "        self.max_sentences = max_sentences\n",
        "        self.output_size = output_size\n",
        "        self.embedding_matrix = embedding_matrix\n",
        "        self.word_encoding_dim = word_encoding_dim\n",
        "        self.sentence_encoding_dim = sentence_encoding_dim\n",
        "\n",
        "\n",
        "        in_tensor, out_tensor = self._build_network()\n",
        "\n",
        "        super(HAN, self).__init__(\n",
        "            inputs=in_tensor, outputs=out_tensor, name=name\n",
        "        )\n",
        "    def build_word_encoder(self, max_words, embedding_matrix, encoding_dim=200):\n",
        "        \"\"\"\n",
        "        Build the model that embeds and encodes in context the\n",
        "        words used in a sentence. The return model takes a tensor of shape\n",
        "        (batch_size, max_length) that represents a collection of sentences\n",
        "        and returns an encoded representation of these sentences.\n",
        "        :param max_words: (int) The maximum sentence length this model accepts\n",
        "        :param embedding_matrix: (2d array-like) A matrix with the i-th row\n",
        "            representing the embedding of the word represented by index i.\n",
        "        :param encoding_dim: (int, should be even) The dimension of the\n",
        "            bidirectional encoding layer. Half of the nodes are used in the\n",
        "            forward direction and half in the backward direction.\n",
        "        :return: Instance of keras.Model\n",
        "        \"\"\"\n",
        "        assert encoding_dim % 2 == 0, \"Embedding dimension should be even\"\n",
        "\n",
        "        vocabulary_size = embedding_matrix.shape[0]\n",
        "        embedding_dim = embedding_matrix.shape[1]\n",
        "\n",
        "        embedding_layer = Embedding(\n",
        "            vocabulary_size, embedding_dim,\n",
        "            weights=[embedding_matrix], input_length=max_words,\n",
        "            trainable=False\n",
        "        )\n",
        "\n",
        "        sentence_input = Input(shape=(max_words,), dtype='int32')\n",
        "        embedded_sentences = embedding_layer(sentence_input)\n",
        "        encoded_sentences = Bidirectional(\n",
        "            GRU(int(encoding_dim / 2), return_sequences=True)\n",
        "        )(embedded_sentences)\n",
        "\n",
        "        return Model(\n",
        "            inputs=[sentence_input], outputs=[encoded_sentences], name='word_encoder'\n",
        "        )\n",
        "    def build_sentence_encoder(self, max_sentences, summary_dim, encoding_dim=200):\n",
        "        \"\"\"\n",
        "        Build the encoder that encodes the vector representation of\n",
        "        sentences in their context.\n",
        "        :param max_sentences: The maximum number of sentences that can be\n",
        "            passed. Use zero-padding to supply shorter sentences.\n",
        "        :param summary_dim: (int) The dimension of the vectors that summarizes\n",
        "            sentences. Should be equal to the encoding_dim of the word\n",
        "            encoder.\n",
        "        :param encoding_dim: (int, even) The dimension of the vector that\n",
        "            summarizes sentences in context. Half is used in forward direction,\n",
        "            half in backward direction.\n",
        "        :return: Instance of keras.Model\n",
        "        \"\"\"\n",
        "        assert encoding_dim % 2 == 0, \"Embedding dimension should be even\"\n",
        "\n",
        "        text_input = Input(shape=(max_sentences, summary_dim))\n",
        "        encoded_sentences = Bidirectional(\n",
        "            GRU(int(encoding_dim / 2), return_sequences=True)\n",
        "        )(text_input)\n",
        "        return Model(\n",
        "            inputs=[text_input], outputs=[encoded_sentences], name='sentence_encoder'\n",
        "        )\n",
        "\n",
        "    def _build_network(self):\n",
        "        \"\"\"\n",
        "        Build the graph that represents this network\n",
        "        :return: in_tensor, out_tensor, Tensors representing the input and output\n",
        "            of this network.\n",
        "        \"\"\"\n",
        "        in_tensor = Input(shape=(self.max_sentences, self.max_words))\n",
        "\n",
        "        word_encoder = self.build_word_encoder(\n",
        "            self.max_words, self.embedding_matrix, self.word_encoding_dim\n",
        "        )\n",
        "\n",
        "        word_rep = TimeDistributed(\n",
        "            word_encoder, name='word_encoder'\n",
        "        )(in_tensor)\n",
        "\n",
        "        # Sentence Rep is a 3d-tensor (batch_size, max_sentences, word_encoding_dim)\n",
        "        sentence_rep = TimeDistributed(\n",
        "            AttentionLayer(), name='word_attention'\n",
        "        )(word_rep)\n",
        "\n",
        "        doc_rep = self.build_sentence_encoder(\n",
        "            self.max_sentences, self.word_encoding_dim, self.sentence_encoding_dim\n",
        "        )(sentence_rep)\n",
        "\n",
        "        # We get the final representation by applying our attention mechanism\n",
        "        # to the encoded sentences\n",
        "        doc_summary = AttentionLayer(name='sentence_attention')(doc_rep)\n",
        "\n",
        "        out_tensor = Dense(\n",
        "            self.output_size, activation='softmax', name='class_prediction'\n",
        "        )(doc_summary)\n",
        "\n",
        "        return in_tensor, out_tensor\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'max_words': self.max_words,\n",
        "            'max_sentences': self.max_sentences,\n",
        "            'output_size': self.output_size,\n",
        "            'embedding_matrix': self.embedding_matrix,\n",
        "            'word_encoding_dim': self.word_encoding_dim,\n",
        "            'sentence_encoding_dim': self.sentence_encoding_dim,\n",
        "            'base_config': super(HAN, self).get_config()\n",
        "        }\n",
        "\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config, custom_objects=None):\n",
        "        \"\"\"\n",
        "        Keras' API isn't really extendible at this point\n",
        "        therefore we need to use a bit hacky solution to\n",
        "        be able to correctly reconstruct the HAN model\n",
        "        from a config. This therefore does not reconstruct\n",
        "        a instance of HAN model, but actually a standard\n",
        "        Keras model that behaves exactly the same.\n",
        "        \"\"\"\n",
        "        base_config = config.pop('base_config')\n",
        "\n",
        "        return Model.from_config(\n",
        "            base_config, custom_objects=custom_objects\n",
        "        )\n",
        "\n",
        "    def predict_sentence_attention(self, X):\n",
        "        \"\"\"\n",
        "        For a given set of texts predict the attention\n",
        "        weights for each sentence.\n",
        "        :param X: 3d-tensor, similar to the input for predict\n",
        "        :return: 2d array (num_obs, max_sentences) containing\n",
        "            the attention weights for each sentence\n",
        "        \"\"\"\n",
        "        att_layer = self.get_layer('sentence_attention')\n",
        "        prev_tensor = att_layer.input\n",
        "\n",
        "        # Create a temporary dummy layer to hold the\n",
        "        # attention weights tensor\n",
        "        dummy_layer = Lambda(\n",
        "            lambda x: att_layer._get_attention_weights(x)\n",
        "        )(prev_tensor)\n",
        "\n",
        "        return Model(self.input, dummy_layer).predict(X)\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHvvHc1pPmqu"
      },
      "source": [
        "**Model Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EVB2XE0Pic8",
        "outputId": "a12ccc51-a0fa-489a-9505-aff1b6300cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "logger.info(\"Training the model.\")\n",
        "\n",
        "\n",
        "han_model = HAN(\n",
        "    MAX_WORDS_PER_SENT, MAX_SENT, 2, embedding_matrix,\n",
        "    word_encoding_dim=100, sentence_encoding_dim=100\n",
        ")\n",
        "\n",
        "han_model.summary()\n",
        "\n",
        "han_model.compile(\n",
        "    optimizer='adagrad', loss='categorical_crossentropy',\n",
        "    metrics=['acc']\n",
        ")"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-09 12:03:34,383 - default - INFO - Training the model.\n",
            "Model: \"han-for-docla\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 15, 100)]         0         \n",
            "_________________________________________________________________\n",
            "word_encoder (TimeDistribute (None, 15, 100, 100)      8196000   \n",
            "_________________________________________________________________\n",
            "word_attention (TimeDistribu (None, 15, 100)           10100     \n",
            "_________________________________________________________________\n",
            "sentence_encoder (Functional (None, 15, 100)           45600     \n",
            "_________________________________________________________________\n",
            "sentence_attention (Attentio (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "class_prediction (Dense)     (None, 2)                 202       \n",
            "=================================================================\n",
            "Total params: 8,262,002\n",
            "Trainable params: 111,602\n",
            "Non-trainable params: 8,150,400\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsiLZOjQPtTi",
        "outputId": "91adf6bc-c939-4708-87db-45959d78ffd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "han_model.fit(\n",
        "    X_train, y_train, batch_size=20, epochs=10,\n",
        "    validation_data=(X_test, y_test)\n",
        ")"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1000/1000 [==============================] - 541s 541ms/step - loss: 0.6942 - acc: 0.5065 - val_loss: 0.6940 - val_acc: 0.4954\n",
            "Epoch 2/10\n",
            "1000/1000 [==============================] - 547s 547ms/step - loss: 0.6926 - acc: 0.5135 - val_loss: 0.6927 - val_acc: 0.5142\n",
            "Epoch 3/10\n",
            "1000/1000 [==============================] - 547s 547ms/step - loss: 0.6919 - acc: 0.5314 - val_loss: 0.6918 - val_acc: 0.5402\n",
            "Epoch 4/10\n",
            "1000/1000 [==============================] - 548s 548ms/step - loss: 0.6913 - acc: 0.5606 - val_loss: 0.6910 - val_acc: 0.5586\n",
            "Epoch 5/10\n",
            "1000/1000 [==============================] - 551s 551ms/step - loss: 0.6907 - acc: 0.5865 - val_loss: 0.6904 - val_acc: 0.6054\n",
            "Epoch 6/10\n",
            "1000/1000 [==============================] - 552s 552ms/step - loss: 0.6902 - acc: 0.5985 - val_loss: 0.6899 - val_acc: 0.6184\n",
            "Epoch 7/10\n",
            "1000/1000 [==============================] - 551s 551ms/step - loss: 0.6896 - acc: 0.6033 - val_loss: 0.6894 - val_acc: 0.5842\n",
            "Epoch 8/10\n",
            "1000/1000 [==============================] - 550s 550ms/step - loss: 0.6891 - acc: 0.5935 - val_loss: 0.6885 - val_acc: 0.6186\n",
            "Epoch 9/10\n",
            "1000/1000 [==============================] - 548s 548ms/step - loss: 0.6885 - acc: 0.5964 - val_loss: 0.6878 - val_acc: 0.6150\n",
            "Epoch 10/10\n",
            "1000/1000 [==============================] - 547s 547ms/step - loss: 0.6879 - acc: 0.6015 - val_loss: 0.6875 - val_acc: 0.5938\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f2781cf5b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    }
  ]
}